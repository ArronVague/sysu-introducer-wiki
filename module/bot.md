# 大语言模型

大语言模型模块是数字人系统的核心组件，
它充当数字人的大脑，负责回答消息。
在本系统中，为了增强大语言模型回答垂直领域的能力，
我们构建了相关的知识数据库，
通过结合**检索器**，**提示工程**等技术，
实现对该模块的能力扩充。

此外，考虑到该模块可能有多种大语言模型调用和检索器的实现，
如果两两组合可能会导致最终实现模块激增，
因此在此我们引入模块化的思想，
将两者封装成子模块，可以在运行时切换不同的实现类型。
对于`caller`和`searcher`子模块来说，
其只需要实现调用和检索的逻辑，
而`bot`模块则负责封装调用这两个子模块进行回答消息的逻辑。

值得注意的是，
本模块并不负责对接受的消息进行筛，
这部分由 DynamicMessageQueue 负责。

<!-- ## 功能

-   大模型内核: 此部分可以通过调用 API 或者使用本地部署的大模型，
    是整个模块中最基础的部分。
-   外部知识库：外部知识库则是通过关系型/向量数据库存储，
    并通过消息对知识库进行检索，提取出最关键的知识信息。
-   长期记忆（Optional）：大模型的每次生成结果都是有意义的，
    因此这些回答可以利用在未来的生成当中。长期记忆就是一个缓存，
    当问题命中时，就可以使用此处的缓存进行生成。
-   短期记忆（Optional）：用户在近期回答的问题会被标记，
    如果继续由相同的问题，大模型可能会表示厌烦的情绪。

> 这里的记忆系统可能和 DMQ 优点重合，
> 但是还是有所区别的。此处的记忆模块，更偏向于大脑的海马体，
> 而 DMQ 更偏向于人眼的注意力机制。
>
> 当然，后续可能将 DMQ 融入大语言模型内核中。 -->

## 处理逻辑

> TODO: 如何使用两个子模块生成消息的回答...

## 子模块

在该模块中， 主要通过 `Bot` 对象来提供回答生成的功能 ，
而它又包含`caller` 和 `searcher`两个子模块，
前者用于封装调用外部 API 或者调用本地 LLM 的逻辑，
而后者主要是作为检索器，用于对相关知识的检索。
`Bot` 的主要处理逻辑封装在 `talk` 函数内部，
它会根据输入的用户 query 从 `searcher` 中查找到相似的文本，
然后在通过 `caller` 生成最终的回答

### 调用器 Caller

目前本项目仅支持如下几种 LLM 的 API 的调用方式，
并可以通过配置文件进行动态加载和配置。

#### 1. GPT

使用 openai sdk 进行调用 GPT3.5。

-   `apiKey`: 调用 API 所需要的秘钥
-   `url`: openai 代理的链接。具体使用说明可以参考 [GPT_API_free](http://github.com/chatanywhere/GPT_API_free)

#### 2. Virtual

如果使用 LLM 模块进行生成文本，会在使用过程中产生相应的费用。
因此，如果在项目开发时，并不特别需要大模型的文本生成的能力，
可以使用此处的 `VirtualBot`，以减少开发调试过程中产生不必要的 token 开销。

该部分提供两种生成回答的策略。第一种随机策略，从随机回答库中选择一条文本进行输出。
第二种是复读策略，会直接输出“我回答了 XXX”。

-   `delay`: 用于控制延迟
-   `isRandom`: 选择生成答案的策略

### 检索器 Searcher

#### 1. ElasticSearch

> TODO: ...
